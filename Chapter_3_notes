# Chapter 3: A semantic theory of information

## Core theorem

For a signal to carry the information that s is F, it must be the case that;

1. The signal carries as much information about s as as would be generated by s's being F.
2. s is F
3. The quantity of information the signal carries about s (or includes) that quantity generated by s's being F (and not s being G).

Where 3) is meant to include that the signal has to have the capacity of carrying that s is F and that it actually does carry that information.

So that the definition of information content is:

**A signal r, carrys the information that s is F is equivalent to saying that the conditional probability of s's being F, given r (and k) is 1, (but, given k alone is less than 1).**

Where k is all prior knowledge about the possibilities that exist at the source. The example used was if a signal is known to be red or blue, and the message says it is not blue, then you know it is red but only if you know red was the only other choice, if you thought green was a valid option then you would none the wiser. k also descibes the relativisim of informational content. As each receiver of a signal may have different prior knowledge ie differing values for k, so that while one receiver can get a conditional probability of 1 another may fall short.

Note that the informational content of a signal is being expressed in the form "s is F" refers to some item at source (p. 66). It is also meant as a de re sense, namely it is about the fact that this s is F, not that all or another s is F (the de dicto sense). For Dretske, a signal's de re iformation is determined by 1) the individual s about which the signal carries information and 2) The information (determined by the open sentence "... is F" it carries about that individual.

Dretske raises what might be objecton the definition of informational content by discussing what happens in a situation such that;

P(A|r) = .07
P(B|r) = .80
P(C|r) = .07
P(D|r) = .06

So the best that can be said of r is that it is probably B. Dretske argues this is not a bug, rather it is a feature as this is how we usually express the information we receive.

*Note to self but this objection that s is B does not have a conditional probability of 1. So I might ageee it is the usage, how does it fulfil Dretske's definition for information content. I think this might be covered by the discussion below, not that Drekste joined the dots. *

This is not elaborated upon, instead we move on to the idea of nested information, where;

The information that t is G is entirely nested within s being F, so that s is F also carries the information that t is G. Further a signal may contain many units of information, that have different meanings, either overtly or by being nested wtihin another piece of information. The latter is analytically nested meaning within information. Nomically (following the laws of nature) the statement that s is F, even if this obeys a law of ature does not automatically imply that t is G as in this de re instance it might not (as yet) because the conditional probability is not 1.

Dretske digresses on p.73 to discuss correlation and to distinguish that given correlation does not imply causation, namely there may not be a unity conditional probability that if F is true, therefore G must also be true in either a de dicto or de re sense, that is H might be nested within F but if it is present then G is nested within it, but not because F is true.

This is intentionality is that s is F can not be substitued by s is H without changing the meaning. Underpinning information intentionality is nomic regularities. Namely that s is F is a manifestation of a law of nature. 


