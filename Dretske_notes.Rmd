---
title: "Dretske"
subtitle: "Knwowledge and the flow of information"
author: "Dr Greig Russell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
   - \usepackage{amsthm}
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_section: true
    citation_package: natbib
bibliography: Reference.bib
link-citations: yes
lof: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

# Chapter 2 - Communication and Information

There is an obvious distinction  between the message within the signal and the signal itself. Yet one can tell something of the message from the signal, With the analogy being made that a bucket can not tell you what is in the bucket but does tell you what can be in the bucket.

Dretske (p.41) makes the point that communication theory does not address what specific information the message holds, rather the focus is on the max amount of information the message could hold. In this he quotes Shannon. *note to self some authors disagree with Shannon's interpretation of his own theory*.

Communication theory is focused on the average amount of information contained in a message (entropy). Not the amount of information contained in a particular message.

Dretske (p.42) makes the distinction between semantic meaning and information. The former is derived from the latter, and although often one and the same, not necessarily so. Dretske uses the example of bridge bidding convention, where the semantic meaning is declared but not what information it contains in this particular instance is not. He makes the point that the information contained in a message can exceed the literal semantic meaning of a message.

**Information is that commodity capable of yielding knowledge and what information contained in a signal is what we can learn from it** (p.44)

The but to that is for Dretske argues that *false* information, as it does not yield verifiable knowledge, is not a true member of the set of true knowledge, *note to self, surely this can only be determined either by hindsight or by reference to prior to learning - very loosely defined*.

### Key argument (p.46)

1. Information is a semantic idea.
2. Semantics is the study of meaning.
3. Communication is not a theory of meaning.

Hence

4. Communication theory is not a theory of information

**Dretske's counter**
That is a theory does provide a satisfactory account of meaning, it does not provide a satisfactory account of any semantic concept.

For Dretske, while there is some logic for a communication engineer trying to right size a communication channel to talk about about the entropy of a system, there is no such concept of average information in two messages on two different topics.

He makes the argument for a pre-arranged system where a 1 is transmitted if the chess piece is on square 2, but 0 otherwise. Communication theory says this is a one bit message, but if it is on square 2 it contains 6 bits of information. Of course, if it is not on square 2 then it contains only 0.05 bits of information. So on average our message contains only 0.095 bits of information.

For a communication engineer this is a highly inefficient system, and a better solution is to give up ever knowing where the chess piece ever actually is but reducing the possibilities of where it might be. If you are only interested in knowing when the chess piece is on square 2, then you are actually much worse off with the more efficent communication system as you get less information all of the time in the communication, even if the channel capacity is greater.

Dretske, however, argues that Communication theory can be used to estimate the quantity of information contained in a given signal (p. 51)

He argues that key are the surprisal of a given message or the amount of information contained in a given message.


\begin{equation}
I(S_i) = - log_2 \frac{1}{p(s_a)}
\end{equation}

and the amount of information contained in r_{a} about s_{a} is given by;

\begin{equation}
I_s(r_a) = I(s_a) - E(r_a)
\end{equation}

where

\begin{equation}
E(r_a) = - \sum P(s_i|r_a) . log_2 P(s_i|r_a)
\end{equation}

Dretske emphasises that this is a non-standard use of the Communication theory to determine the information contained in a given message as opposed to the entropy (average amount of information) that is possible for messages using a particular channel. **Dretske (p.53-54) argues that to determine the absolute amount of information in a given message is challenging, given the need to determine a infinite set of possible circumstances. Instead he argues we can more practically determine the comparitive amount of information between different messages**.

Key is the need to know that r_{a} carries as much information as was contained in s_{a} ie. that equivication was zero. In comparison, if equivication is not zero, it contains r_{a} contains less information than was in s_{a}. It does not matter that we can not define or even know that equivication has occurred, for Dretske information is only about whether the information received was the same or less than the information sent. Belief of the recipent is irrelevant to whether equivication actually occurred.

*Note to self p.56 carries an interesting link to Hume and humans being trapped in a circle of one's own perceptions.*

## Xerox principle of information transmission.

1. If message A carries the information contained in message B
2. and message B carries the information contained in message C
Hence
then message A carries the same information as contained in message C

For Dretske, the Xerox principle is fundamental to information flow. He makes the point (p.60) that message C may contain extra noise, so cumulative losses due to equivication can occur at each step but message A could still contain the same total information. Indeed even if the message is not complete any correlation between message C and message A would be enough so that information is still preserved. A message does not have to tell us everything to tell us something (p.61)

The message however muct have sufficient capacity to carry the information. So loss of bits (ie. capacity) degrades the message absolutely while loss of content may not degrade the information.

# Chapter 3: A semantic theory of information

## Definition

For a signal to carry the information that s is F, it must be the case that;

1. The signal carries as much information about s as as would be generated by s's being F.
2. s is F
3. The quantity of information the signal carries about s (or includes) that quantity generated by s's being F (and not s being G).

Where 3) is meant to include that the signal has to have the capacity of carrying that s is F and that it actually does carry that information.

So that the definition of information content is:

**A signal r, carrys the information that s is F is equivalent to saying that the conditional probability of s's being F, given r (and k) is 1, (but, given k alone is less than 1).**

Where k is all prior knowledge about the possibilities that exist at the source. The example used was if a signal is known to be red or blue, and the message says it is not blue, then you know it is red but only if you know red was the only other choice, if you thought green was a valid option then you would none the wiser. k also descibes the relativisim of informational content. As each receiver of a signal may have different prior knowledge ie differing values for k, so that while one receiver can get a conditional probability of 1 another may fall short.

Note that the informational content of a signal is being expressed in the form "s is F" refers to some item at source (p. 66). It is also meant as a de re sense, namely it is about the fact that this s is F, not that all or another s is F (the de dicto sense). For Dretske, a signal's de re iformation is determined by 1) the individual s about which the signal carries information and 2) The information (determined by the open sentence "... is F" it carries about that individual.

Dretske raises what might be objecton the definition of informational content by discussing what happens in a situation such that;

P(A|r) = .07

P(B|r) = .80

P(C|r) = .07

P(D|r) = .06

So the best that can be said of r is that it is probably B. Dretske argues this is not a bug, rather it is a feature as this is how we usually express the information we receive.

*Note to self but this objection that s is B does not have a conditional probability of 1. So I might ageee it is the usage, how does it fulfil Dretske's definition for information content. I think this might be covered by the discussion below, not that Drekste joined the dots. *

This is not elaborated upon, instead we move on to the idea of nested information, where;

The information that t is G is entirely nested within s being F, so that s is F also carries the information that t is G. Further a signal may contain many units of information, that have different meanings, either overtly or by being nested wtihin another piece of information. The latter is analytically nested meaning within information. Nomically (following the laws of nature) the statement that s is F, even if this obeys a law of ature does not automatically imply that t is G as in this de re instance it might not (as yet) because the conditional probability is not 1.

Dretske digresses on p.73 to discuss correlation and to distinguish that given correlation does not imply causation, namely there may not be a unity conditional probability that if F is true, therefore G must also be true in either a de dicto or de re sense, that is H might be nested within F but if it is present then G is nested within it, but not because F is true.

This is intentionality is that s is F can not be substitued by s is H without changing the meaning. Underpinning information intentionality is nomic regularities. Namely that s is F is a manifestation of a law of nature. 

# Chapter 4: Knowledge

## Traditional definition
The traditional answer to the what is knowledge question, is that it is a justified true belief. This is usually seens as a two step process. The first is that the belief is justified and that s is F.

This traditional view has a number of challenges and Dretske proposes to replace this with an information centric model. There is to be three stages to this;

1. What is knowledge?
2. What is knowledge in terms of information and belief?
3. Belief is then to  be resolved into its informational components.

**Key Premise**
Knowledge is considered from the perspective of perceptual knowledge.  

## Information centric definition
**K knows that *s* is *F*, equals *K* belief that *s* is caused (or causally sustained) by the information that *s* is *F*.**

where knowledge about that something is s is due to something other than prior belief or knowledge and is confinded to the perceptual object and the source from where this information was received. Any belief that K might have that s is F is onl knowledge if and only if that belief is caused by the information that *s* is *F*.

**In summary**, knowledge is identified with an information-produced (or sustained) belief, but the information a person receives is relative  to what they already know about the possibilities of the source. So whether a person can learn that s is F, may depend on prior knowedge about s but does not depend on knowing that s is F.

What is meant by saying that a belief is caused by a piece of information? Supposed signal r carries the information that s is F, but carries that information due to having the property F'. There is signal carries the information that s is F by virtue of having the property F', when it is the signal being F' that carries the information, then and only then will we that the information s is F causes whatever the signal's being F' causes.

Dretske uses the example of a window breaking after being hit by an object.  If the object is a brick then it has the property of velocity and weight, so is having the property of momentum that underpins the idea that the brick broke the window. WHile a leaf does not have the property of momentum, so can not underpin the knowledge that the window broke after being hit by the leaf(p.88).

Dretske then argues for how information causally sustains a belief. He uses the example of **K** who is reassured by a meddler that *s* was *F*. The meddler knows nothing on the subject but **K** believes him. **K** then observes that indeed *s* is *F*. In which case, **K** knows that *s* is *F* because of the information derived from his observations not because of his false beliefs derived from the meddler (p. 89). 

For Dretske this information derived from observations is the "second string" that supports a prior belief in say *s* is *F*. Neither is seen as necessary, as each alone is sufficient. The second "string" is helping and it *sufficies* for support of the object, and is what Dretske means by a "sustaining cause".  Yet, the observation that *s* is *F* is insufficent for it be a "supporting cause" of K's belief, if it merely supports the false belief from the mddler (p. 90). 

*Note to self this all seems just so faffed up?*

This links back to the definition about thet *s* is *F* only because it has the same property as *F'*. So observation alone that *s* is *F* with the presence of *F'*, such as the false reassurance of the meddler, is not sufficient for **K** to have verifiable knowledge derived from information is *s* is *F*. **K** may believe there is a fault with his sensory systems so that although *s* is *F* is observed, it does not engender the belief that *s* is *F* for **K**. 

*Note to self, so how does a beliefe get formed?*

## Definition of knowledge
Rather, this is "coordination between our ordinary concept of knowledge (or, better, perceptual knowledge) and the technical idea of informaiton".

### Ordinary Judgments
For **Y** to be necessary for **X**, there needs not to be any situation where **X** occurs without **Y**. If not, there is inductive grounds for arguing that **Y** is a necessary condtion for **X**. 

The example given is the memo with Herman's name written on it being lost. The careless messanger, writes another and by chance alone writes Herman's name. The manager therefore has no true knowledge that Herman was the employee chosen for the unpleasant task by the workforce.

Similarly, imagine a jar of coloured bals with the following distribution of balls;

P(P|r) = 0.9

P(Y|r) = 0.03

P(B|r) = 0.03

P(G|r) = 0.04

The signal raises the probability that *s* is *P* but the equivication is 0.6 bits, and only 1.4 bits of information, so the receiver of the signal can never be sure the answer will be pink, because the equivication is not zero. Hence although the receiver might be expecting the ball to be P=Pink, the signal said that the ball was P=Pink but he could never know for 100% as the condition probability is not 1, and the equivication is not 0 (p. 94). So there is no ability to achieve **certainty knwledge**, ie knowledge where there is 0% equivication associated with the information in the message.

If the receiver was then told that someone saw the ball being drawn and it was not Yellow, Green or Blue, then the receiver could reasonably assume the sum of the two bits of information means that the ball was Pink, now he is able to **know**, this extra information or *k*  from the earlier definition enables **certain knowledge** to be achieved.

## The Gettier Problem
Dretske goes on to consider the "Gettier problem", namely that if knowledge is a justified true belief the challenge of being able to have a justified true belief about something is false, is a problem. 

*Note to self, more details about the Gettier problem are needed*

Information derived **certain knowledge** provides an escape. *K* might have a justified true belief the *s* is *T* but the information that *s* is *T* was never within the signal, because *s* is *F*. Hence *K* can never have **certain knowledge** that *s* is *T* but the belief that it is true is fully justified. For Dretske then, there is a clear difference between the belief that *s* is *T* and the knowledge *s* is actually *F*. The former is a justified true belief, that information is not in the signal, ie. the equivication is not xero, and the latter which is **certain knowledge** as there is no equivication associated with the signal.

## The Lottery Paradox
In a given lottery the chances of holding a winning ticket is very very small, but not zero. So, with out information that *s* is *F*, but only s, one could assume that *K* knows she is not holding a winning ticket. Suppose the associated equvication is $e_F$. Likewise, *K* would know that a friend also had a ticket, that is unlikely to win, so *K* knows that *t* is *G*, with the associated small equivication being $e_G$. By the conjunction principle, *K* knows that the total equivication is $e_F + e_G$. Hence the more unlucky friends that *K* has, the more the total equivication known rises;

\begin{equation}
e_T = \sum_{i = 0}{i = n)} e_i
\end{equation}

The total equivication known to *K* grows the more she collects friends who do not know that *s* is *F*. Hence as *K* has not information that *s* is *F* only a slight belief that *s* is *F*, she has no certain knowledge of any outcome and the cumulative rule does not apply.

## Chinese whispers
Dretske then raises the challenge of what used to be called Chinese whispers. In this he imagines an infinite chain of individuals from $K_1$ to $K_n$. $K_1$ tells $K_@$ than *s* is *F*, with a very small equivication. So small it does not distort the intent of the message. As the message is passed down the chain, the equivication builds to the point that the message is not lost but the information underpinning that message has a conditional probability no better than chance. For Dretske the key here is not the loss of information, rather there is no awareness of the point at which the information the *s* is *F* was lost.

Hence Dretske argues that with any equivication, **certain knowledge** is immediately lost.

